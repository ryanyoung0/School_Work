{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import validation_curve, GridSearchCV\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and plot the famous moons dataset from sklearn\n",
    "xdat, ydat = datasets.make_moons(n_samples=200, noise=0.25, random_state=2)\n",
    "fig0, ax0 = plt.subplots()\n",
    "ax0.scatter(xdat[:,0], xdat[:,1], c=1-ydat, cmap='coolwarm') # set color to c=1-ydat to reverse colors of the moons\n",
    "ax0.set(xlabel='x1', ylabel='x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore classification using rbf kernel by varying hyperparameters\n",
    "clf = SVC(C=1, kernel='rbf', gamma=1)\n",
    "clf.fit(xdat, ydat)\n",
    "\n",
    "# set ranges for plots\n",
    "meshstep = 0.01\n",
    "x1_min, x1_max = xdat[:,0].min() - .5, xdat[:,0].max() + .5\n",
    "x2_min, x2_max = xdat[:,1].min() - .5, xdat[:,1].max() + .5\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, meshstep), np.arange(x2_min, x2_max, meshstep))\n",
    "\n",
    "# plot the decision function, which is proportional to the distance from the decision boundary\n",
    "# (lighter colors cover points closer to the decision boundary)\n",
    "z = clf.decision_function(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "z = z.reshape(xx1.shape)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.scatter(xdat[:,0], xdat[:,1], c=1-ydat, cmap='coolwarm')\n",
    "ax1.set(xlabel='x1', ylabel='x2')\n",
    "ax1.contourf(xx1, xx2, z, cmap=plt.cm.RdBu, alpha=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN THE FOLLOWING SECTION, WE WILL TRAIN RBF KERNEL SVC USING FIXED C=1\n",
    "\n",
    "# (Note: n_jobs has been added to speed up calculations via parallelization, \n",
    "# you may adjust this based on your processor or default to 1; \n",
    "# verbose has also been added to print out information while the function is running.)\n",
    "clf_fixedC = SVC(C=1, kernel='rbf')\n",
    "gammalist = np.logspace(-3,3,num=7)\n",
    "train_scores, valid_scores = validation_curve(clf_fixedC, xdat, ydat, 'gamma', gammalist, cv=10, n_jobs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean training and validation errors over k folds for each gamma\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "valid_scores_mean = np.mean(valid_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot training and validation error as a function of gamma\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(gammalist, 1-train_scores_mean, 'k-o', label='training')\n",
    "ax2.plot(gammalist, 1-valid_scores_mean, 'r-o', label='validation')\n",
    "ax2.set(xlabel='gamma', ylabel='1-accuracy', xscale='log', ylim=[0,1], title='rbf kernel SVC')\n",
    "ax2.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN THE FOLLOWING SECTION, WE WILL USE GRID SEARCH OVER C AND GAMMA, WITH 10-FOLD CV\n",
    "\n",
    "# (Note: n_jobs has been added to speed up calculations via parallelization, \n",
    "# you may adjust this based on your processor or default to 1; \n",
    "# verbose has also been added to print out information while the function is running.)\n",
    "Clist = np.logspace(-8,6,num=8)\n",
    "gammalist = np.logspace(-3,3,num=7)\n",
    "parameters = {'C':Clist, 'gamma':gammalist}\n",
    "clf_gridsearch = GridSearchCV(SVC(kernel='rbf'), parameters, cv=10, n_jobs=4, verbose=1)\n",
    "clf_gridsearch.fit(xdat, ydat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best hyperparameters and plot the heat map of validation errors\n",
    "print(\"The best parameters are %s with a validation accuracy of %0.2f\" % (clf_gridsearch.best_params_, clf_gridsearch.best_score_))\n",
    "\n",
    "scores = clf_gridsearch.cv_results_['mean_test_score'].reshape(len(Clist),len(gammalist))\n",
    "fig3, ax3 = plt.subplots()\n",
    "currentimage = ax3.imshow(scores, cmap=plt.cm.hot)\n",
    "ax3.set(xlabel='gamma', ylabel='C', title='validation accuracy')\n",
    "fig3.colorbar(currentimage)\n",
    "plt.xticks(np.arange(len(gammalist)), gammalist, rotation=45)\n",
    "plt.yticks(np.arange(len(Clist)), Clist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
